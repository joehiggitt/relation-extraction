{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\joehi\\Documents\\Git\\relation-extraction\\.conda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, Bidirectional, LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = r\"C:\\Users\\joehi\\Documents\\Work\\Uni - Year 4\\COMP61332\\Coursework\\dataset\\data\\json\"\n",
    "# FILE = r\"\\train.json\"\n",
    "FILE = r\"\\dev.json\"\n",
    "\n",
    "df = pd.read_json(BASE_PATH + FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>docid</th>\n",
       "      <th>relation</th>\n",
       "      <th>token</th>\n",
       "      <th>subj_start</th>\n",
       "      <th>subj_end</th>\n",
       "      <th>obj_start</th>\n",
       "      <th>obj_end</th>\n",
       "      <th>subj_type</th>\n",
       "      <th>obj_type</th>\n",
       "      <th>stanford_pos</th>\n",
       "      <th>stanford_ner</th>\n",
       "      <th>stanford_head</th>\n",
       "      <th>stanford_deprel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e7798fb926b9403cfcd2</td>\n",
       "      <td>APW_ENG_20101103.0539</td>\n",
       "      <td>per:title</td>\n",
       "      <td>[At, the, same, time, ,, Chief, Financial, Off...</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>TITLE</td>\n",
       "      <td>[IN, DT, JJ, NN, ,, NNP, NNP, NNP, NNP, NNP, M...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, PERSON, PERSON, O, O,...</td>\n",
       "      <td>[4, 4, 4, 12, 12, 10, 10, 10, 10, 12, 12, 0, 1...</td>\n",
       "      <td>[case, det, amod, nmod, punct, compound, compo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e779865fb96bbbcc4ca4</td>\n",
       "      <td>APW_ENG_20080229.1401.LDC2009T13</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[U.S., District, Court, Judge, Jeffrey, White,...</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>[NNP, NNP, NNP, NNP, NNP, NNP, IN, NNP, VBD, D...</td>\n",
       "      <td>[LOCATION, O, O, O, PERSON, PERSON, O, O, O, O...</td>\n",
       "      <td>[6, 6, 6, 6, 6, 9, 8, 6, 0, 11, 9, 13, 11, 20,...</td>\n",
       "      <td>[compound, compound, compound, compound, compo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e7798ae9c0adbcdc81e7</td>\n",
       "      <td>APW_ENG_20090707.0488</td>\n",
       "      <td>per:city_of_death</td>\n",
       "      <td>[PARIS, 2009-07-07, 11:07:32, UTC, French, med...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>CITY</td>\n",
       "      <td>[NNP, CD, CD, NNP, NNP, NNS, RBR, VBD, IN, NNP...</td>\n",
       "      <td>[LOCATION, TIME, TIME, TIME, MISC, O, O, O, O,...</td>\n",
       "      <td>[6, 6, 6, 6, 6, 8, 8, 0, 16, 16, 10, 10, 12, 1...</td>\n",
       "      <td>[compound, nummod, nummod, compound, compound,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e779865fb9bddb4eccbc</td>\n",
       "      <td>LTW_ENG_20070522.0147.LDC2009T13</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[The, current, holdings, of, Blackstone-operat...</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>DATE</td>\n",
       "      <td>[DT, JJ, NNS, IN, JJ, NNS, VBP, NNP, NNP, ,, N...</td>\n",
       "      <td>[O, DATE, O, O, MISC, O, O, ORGANIZATION, ORGA...</td>\n",
       "      <td>[3, 3, 7, 6, 6, 3, 0, 9, 7, 9, 12, 9, 9, 15, 9...</td>\n",
       "      <td>[det, amod, nsubj, case, amod, nmod, ROOT, com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e779865fb97262ce4da3</td>\n",
       "      <td>eng-NG-31-108589-8120474</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[http://groups.yahoo.com/group/aspartameNM/mes...</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>[NN, NN, IN, DT, JJ, JJ, NN, IN, NN, NN, :, NN...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[2, 0, 7, 7, 7, 7, 2, 10, 10, 7, 2, 13, 2, 13,...</td>\n",
       "      <td>[compound, ROOT, case, det, amod, amod, nmod, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                             docid           relation  \\\n",
       "0  e7798fb926b9403cfcd2             APW_ENG_20101103.0539          per:title   \n",
       "1  e779865fb96bbbcc4ca4  APW_ENG_20080229.1401.LDC2009T13        no_relation   \n",
       "2  e7798ae9c0adbcdc81e7             APW_ENG_20090707.0488  per:city_of_death   \n",
       "3  e779865fb9bddb4eccbc  LTW_ENG_20070522.0147.LDC2009T13        no_relation   \n",
       "4  e779865fb97262ce4da3          eng-NG-31-108589-8120474        no_relation   \n",
       "\n",
       "                                               token  subj_start  subj_end  \\\n",
       "0  [At, the, same, time, ,, Chief, Financial, Off...           8         9   \n",
       "1  [U.S., District, Court, Judge, Jeffrey, White,...          17        18   \n",
       "2  [PARIS, 2009-07-07, 11:07:32, UTC, French, med...           9         9   \n",
       "3  [The, current, holdings, of, Blackstone-operat...          13        14   \n",
       "4  [http://groups.yahoo.com/group/aspartameNM/mes...          27        27   \n",
       "\n",
       "   obj_start  obj_end     subj_type obj_type  \\\n",
       "0         12       12        PERSON    TITLE   \n",
       "1          4        5        PERSON   PERSON   \n",
       "2          0        0        PERSON     CITY   \n",
       "3          1        1  ORGANIZATION     DATE   \n",
       "4         53       53  ORGANIZATION   PERSON   \n",
       "\n",
       "                                        stanford_pos  \\\n",
       "0  [IN, DT, JJ, NN, ,, NNP, NNP, NNP, NNP, NNP, M...   \n",
       "1  [NNP, NNP, NNP, NNP, NNP, NNP, IN, NNP, VBD, D...   \n",
       "2  [NNP, CD, CD, NNP, NNP, NNS, RBR, VBD, IN, NNP...   \n",
       "3  [DT, JJ, NNS, IN, JJ, NNS, VBP, NNP, NNP, ,, N...   \n",
       "4  [NN, NN, IN, DT, JJ, JJ, NN, IN, NN, NN, :, NN...   \n",
       "\n",
       "                                        stanford_ner  \\\n",
       "0  [O, O, O, O, O, O, O, O, PERSON, PERSON, O, O,...   \n",
       "1  [LOCATION, O, O, O, PERSON, PERSON, O, O, O, O...   \n",
       "2  [LOCATION, TIME, TIME, TIME, MISC, O, O, O, O,...   \n",
       "3  [O, DATE, O, O, MISC, O, O, ORGANIZATION, ORGA...   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                       stanford_head  \\\n",
       "0  [4, 4, 4, 12, 12, 10, 10, 10, 10, 12, 12, 0, 1...   \n",
       "1  [6, 6, 6, 6, 6, 9, 8, 6, 0, 11, 9, 13, 11, 20,...   \n",
       "2  [6, 6, 6, 6, 6, 8, 8, 0, 16, 16, 10, 10, 12, 1...   \n",
       "3  [3, 3, 7, 6, 6, 3, 0, 9, 7, 9, 12, 9, 9, 15, 9...   \n",
       "4  [2, 0, 7, 7, 7, 7, 2, 10, 10, 7, 2, 13, 2, 13,...   \n",
       "\n",
       "                                     stanford_deprel  \n",
       "0  [case, det, amod, nmod, punct, compound, compo...  \n",
       "1  [compound, compound, compound, compound, compo...  \n",
       "2  [compound, nummod, nummod, compound, compound,...  \n",
       "3  [det, amod, nsubj, case, amod, nmod, ROOT, com...  \n",
       "4  [compound, ROOT, case, det, amod, amod, nmod, ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                              e7798ae9c0adbcdc81e7\n",
      "docid                                          APW_ENG_20090707.0488\n",
      "relation                                           per:city_of_death\n",
      "token              [PARIS, 2009-07-07, 11:07:32, UTC, French, med...\n",
      "subj_start                                                         9\n",
      "subj_end                                                           9\n",
      "obj_start                                                          0\n",
      "obj_end                                                            0\n",
      "subj_type                                                     PERSON\n",
      "obj_type                                                        CITY\n",
      "stanford_pos       [NNP, CD, CD, NNP, NNP, NNS, RBR, VBD, IN, NNP...\n",
      "stanford_ner       [LOCATION, TIME, TIME, TIME, MISC, O, O, O, O,...\n",
      "stanford_head      [6, 6, 6, 6, 6, 8, 8, 0, 16, 16, 10, 10, 12, 1...\n",
      "stanford_deprel    [compound, nummod, nummod, compound, compound,...\n",
      "Name: 2, dtype: object \n",
      "\n",
      "\n",
      "PARIS          \tNNP\tLOCATION       \t6\tcompound\n",
      "2009-07-07     \tCD\tTIME           \t6\tnummod\n",
      "11:07:32       \tCD\tTIME           \t6\tnummod\n",
      "UTC            \tNNP\tTIME           \t6\tcompound\n",
      "French         \tNNP\tMISC           \t6\tcompound\n",
      "media          \tNNS\tO              \t8\tnsubj\n",
      "earlier        \tRBR\tO              \t8\tadvmod\n",
      "reported       \tVBD\tO              \t0\tROOT\n",
      "that           \tIN\tO              \t16\tmark\n",
      "Montcourt      \tNNP\tPERSON         \t16\tnsubjpass\n",
      ",              \t,\tO              \t10\tpunct\n",
      "ranked         \tVBD\tO              \t10\tacl\n",
      "119            \tCD\tNUMBER         \t12\tdobj\n",
      ",              \t,\tO              \t10\tpunct\n",
      "was            \tVBD\tO              \t16\tauxpass\n",
      "found          \tVBN\tO              \t8\tccomp\n",
      "dead           \tJJ\tO              \t16\txcomp\n",
      "by             \tIN\tO              \t20\tcase\n",
      "his            \tPRP$\tO              \t20\tnmod:poss\n",
      "girlfriend     \tNN\tO              \t17\tnmod\n",
      "in             \tIN\tO              \t23\tcase\n",
      "the            \tDT\tO              \t23\tdet\n",
      "stairwell      \tNN\tO              \t16\tnmod\n",
      "of             \tIN\tO              \t27\tcase\n",
      "his            \tPRP$\tO              \t27\tnmod:poss\n",
      "Paris          \tNNP\tLOCATION       \t27\tcompound\n",
      "apartment      \tNN\tO              \t23\tnmod\n",
      ".              \t.\tO              \t8\tpunct\n"
     ]
    }
   ],
   "source": [
    "d = df.iloc[2]\n",
    "print(d, \"\\n\\n\")\n",
    "\n",
    "for i in range(len(d[\"token\"])):\n",
    "    print(d[\"token\"][i].ljust(15), d[\"stanford_pos\"][i], d[\"stanford_ner\"][i].ljust(10), d[\"stanford_head\"][i], d[\"stanford_deprel\"][i], sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "INPUT_SIZE = ...\n",
    "EMEDDING_SIZE = 300\n",
    "LSTM_UNITS = 128\n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "MAX_LEN = 100\n",
    "NUM_LABELS = df[\"relation\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_token_array(input_array, max_len):\n",
    "    mask = np.arange(max_len) < np.array([len(seq) for seq in input_array])[:, None]\n",
    "    padded_array = np.full((len(input_array), max_len), '', dtype='object')\n",
    "    padded_array[mask] = np.concatenate(input_array)\n",
    "    return padded_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "all_tokens = np.concatenate(df['token'].values)\n",
    "tokens = pad_token_array(df['token'].values, MAX_LEN)\n",
    "\n",
    "vectoriser = TextVectorization(max_tokens=VOCAB_SIZE, output_sequence_length=MAX_LEN)\n",
    "vectoriser.adapt(all_tokens)\n",
    "sequences = np.array(vectoriser(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   19     2   655 ...     0     0     0]\n",
      " [   37  1271   342 ...     0     0     0]\n",
      " [  303     1     1 ...     0     0     0]\n",
      " ...\n",
      " [  108    60  1113 ...     0     0     0]\n",
      " [  448   501    15 ...     0     0     0]\n",
      " [14433 13513 15316 ...     0     0     0]] (19584, 100) \n",
      "\n",
      "[[   19     2   655 ...     0     0     0]\n",
      " [   37  1271   342 ...     0     0     0]\n",
      " [  303     1     1 ...     0     0     0]\n",
      " ...\n",
      " [  108    60  1113 ...     0     0     0]\n",
      " [  448   501    15 ...     0     0     0]\n",
      " [14433     0 13513 ...     0     0     0]] (19584, 100)\n",
      "[0.24430147 0.75569853]\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for sentence in df['token'].values:\n",
    "    sentences.append(\" \".join(sentence))\n",
    "\n",
    "sequences2 = np.array(vectoriser(sentences))\n",
    "print(sequences2, sequences2.shape, \"\\n\")\n",
    "sequences = np.array(vectoriser(tokens.reshape(*tokens.shape, 1)))\n",
    "# print(sequences, sequences.shape, \"\\n\")\n",
    "a = sequences[:,:,0]\n",
    "print(a, a.shape)\n",
    "\n",
    "print(np.bincount((sequences2 == a).flatten())/np.bincount((sequences2 == a).flatten()).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LSTM Model\n",
    "\n",
    "The codeblock below implements a Bidirectional LSTM model. The design of this is adapted from https://medium.com/southpigalle/simple-relation-extraction-with-a-bi-lstm-model-part-1-682b670d5e11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelExLSTM(Model):\n",
    "    def __init__(self, input_size, embedding_size, lstm_units, vocab_size, num_labels, max_len, vocab, use_vocab=False):\n",
    "        super(RelExLSTM, self).__init__()\n",
    "\n",
    "        if use_vocab:\n",
    "            self.vectoriser = TextVectorization(vocabulary=vocab)\n",
    "        else:\n",
    "            self.vectoriser = TextVectorization(max_tokens=vocab_size, output_sequence_length=max_len)\n",
    "            self.vectoriser.adapt(vocab)\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, embedding_size, input_length=input_size)\n",
    "        self.lstm = Bidirectional(LSTM(lstm_units, dropout=0.7, recurrent_dropout=0.7))\n",
    "        self.dense = Dense(num_labels, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.vectoriser(inputs)\n",
    "        x = self.embedding(x)\n",
    "        x = self.lstm(x)\n",
    "        y = self.dense(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = RelExLSTM(INPUT_SIZE, EMEDDING_SIZE, LSTM_UNITS, VOCAB_SIZE, NUM_LABELS)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
